{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas_datareader/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     DataReader,\n\u001b[1;32m      7\u001b[0m     Options,\n\u001b[1;32m      8\u001b[0m     get_components_yahoo,\n\u001b[1;32m      9\u001b[0m     get_dailysummary_iex,\n\u001b[1;32m     10\u001b[0m     get_data_alphavantage,\n\u001b[1;32m     11\u001b[0m     get_data_enigma,\n\u001b[1;32m     12\u001b[0m     get_data_famafrench,\n\u001b[1;32m     13\u001b[0m     get_data_fred,\n\u001b[1;32m     14\u001b[0m     get_data_moex,\n\u001b[1;32m     15\u001b[0m     get_data_quandl,\n\u001b[1;32m     16\u001b[0m     get_data_stooq,\n\u001b[1;32m     17\u001b[0m     get_data_tiingo,\n\u001b[1;32m     18\u001b[0m     get_data_yahoo,\n\u001b[1;32m     19\u001b[0m     get_data_yahoo_actions,\n\u001b[1;32m     20\u001b[0m     get_iex_book,\n\u001b[1;32m     21\u001b[0m     get_iex_data_tiingo,\n\u001b[1;32m     22\u001b[0m     get_iex_symbols,\n\u001b[1;32m     23\u001b[0m     get_last_iex,\n\u001b[1;32m     24\u001b[0m     get_markets_iex,\n\u001b[1;32m     25\u001b[0m     get_nasdaq_symbols,\n\u001b[1;32m     26\u001b[0m     get_quote_yahoo,\n\u001b[1;32m     27\u001b[0m     get_recent_iex,\n\u001b[1;32m     28\u001b[0m     get_records_iex,\n\u001b[1;32m     29\u001b[0m     get_summary_iex,\n\u001b[1;32m     30\u001b[0m     get_tops_iex,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m PKG \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m     35\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m get_versions()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas_datareader/data.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecate_kwarg\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mav\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AVForexReader\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mav\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquotes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AVQuotesReader\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mav\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AVSectorPerformanceReader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas_datareader/av/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemoteDataError\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BaseReader\n\u001b[1;32m      8\u001b[0m AV_BASE_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.alphavantage.co/query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas_datareader/_utils.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_datetime\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_datareader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_number\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymbolWarning\u001b[39;00m(\u001b[38;5;167;01mUserWarning\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas_datareader/compat/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as data\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '2020-01-01'\n",
    "end = '2025-05-09'\n",
    "\n",
    "# Get the data\n",
    "df = yf.download('RELIANCE.NS', start=start, end=end)\n",
    "df = df[['Open', 'High', 'Low', 'Close']]\n",
    "df.reset_index(inplace=True)\n",
    "df.set_index('Date', inplace=True)\n",
    "# print(df.head())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df.Close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma100 = df.Close.rolling(100).mean()\n",
    "ma100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df.Close)\n",
    "plt.plot(ma100, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma200 = df.Close.rolling(200).mean()\n",
    "ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.Close)\n",
    "plt.plot(ma100, 'r')\n",
    "plt.plot(ma200, 'g')\n",
    "plt.legend(['Close', 'MA100', 'MA200'])\n",
    "plt.title('Stock Price with 100 and 200 days moving average')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and test set\n",
    "\n",
    "data_training = pd.DataFrame(df['Close'][0:int(len(df)*0.70)])\n",
    "data_testing = pd.DataFrame(df['Close'][int(len(df)*0.70):int(len(df))])\n",
    "\n",
    "print(data_training.shape)\n",
    "print(data_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training_array = scaler.fit_transform(data_training)\n",
    "data_training_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(100, data_training_array.shape[0]):\n",
    "    x_train.append(data_training_array[i-100:i])\n",
    "    y_train.append(data_training_array[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML model\n",
    "\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x_train.shape[1], 1))) \n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(units=60, activation='relu', return_sequences=True, input_shape=(x_train.shape[1], 1))) \n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "# model.add(LSTM(units=80, activation='relu', return_sequences=True, input_shape=(x_train.shape[1], 1))) \n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(LSTM(units=120, activation='relu')) \n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(Dense(units=1))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third LSTM layer\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.fit(x_train, y_train, epochs = 100) #Train the model using the training data\n",
    "\n",
    "# Compile the model\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "# Fit the model to the training data\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('./LSTM_model.h5')\n",
    "model.save('./LSTM_model_adam_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_100_days = data_training.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = past_100_days.append(data_testing, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = scaler.fit_transform(final_df)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(100, input_data.shape[0]):\n",
    "    x_test.append(input_data[i-100:i])\n",
    "    y_test.append(input_data[i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_factor = 1/0.00101725\n",
    "scale_factor = 1/scaler.scale_\n",
    "y_pred = y_pred*scale_factor\n",
    "y_test = y_test*scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test, 'b', label='Original Price')\n",
    "plt.plot(y_pred, 'r', label='Predicted Price')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the LSTM Model Prediction\n",
    "\n",
    "Several strategies can be implemented to improve the model's prediction accuracy:\n",
    "\n",
    "1. **Feature Engineering**: Add more relevant features beyond just the closing price\n",
    "2. **Hyperparameter Tuning**: Optimize model parameters\n",
    "3. **Model Architecture**: Experiment with different LSTM architectures\n",
    "4. **Regularization**: Apply techniques to prevent overfitting\n",
    "5. **Evaluation Metrics**: Use proper metrics to evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Engineering - Adding Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with additional features\n",
    "def add_features(df):\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. Moving Averages\n",
    "    df_features['MA7'] = df['Close'].rolling(window=7).mean()\n",
    "    df_features['MA21'] = df['Close'].rolling(window=21).mean()\n",
    "    \n",
    "    # 2. Standard Deviation (Volatility)\n",
    "    df_features['STD20'] = df['Close'].rolling(window=20).std()\n",
    "    \n",
    "    # 3. Relative Strength Index (RSI)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df_features['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # 4. MACD (Moving Average Convergence Divergence)\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df_features['MACD'] = exp1 - exp2\n",
    "    df_features['Signal_Line'] = df_features['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # 5. Bollinger Bands\n",
    "    df_features['BB_Upper'] = df_features['MA21'] + (df_features['STD20'] * 2)\n",
    "    df_features['BB_Lower'] = df_features['MA21'] - (df_features['STD20'] * 2)\n",
    "    \n",
    "    # 6. Price Rate of Change\n",
    "    df_features['ROC'] = df['Close'].pct_change(periods=5) * 100\n",
    "    \n",
    "    # Forward fill NaN values\n",
    "    df_features = df_features.fillna(method='ffill')\n",
    "    # Backward fill any remaining NaN values\n",
    "    df_features = df_features.fillna(method='bfill')\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "enhanced_df = add_features(df)\n",
    "enhanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install keras-tuner if not already installed\n",
    "# !pip install -q keras-tuner\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune the number of units in the first LSTM layer\n",
    "    hp_units_lstm1 = hp.Int('units_lstm1', min_value=32, max_value=256, step=32)\n",
    "    model.add(LSTM(units=hp_units_lstm1, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    model.add(Dropout(hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Tune the number of units in the second LSTM layer\n",
    "    hp_units_lstm2 = hp.Int('units_lstm2', min_value=16, max_value=128, step=16))\n",
    "    model.add(LSTM(units=hp_units_lstm2, return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Tune the number of units in the third LSTM layer\n",
    "    hp_units_lstm3 = hp.Int('units_lstm3', min_value=8, max_value=64, step=8))\n",
    "    model.add(LSTM(units=hp_units_lstm3))\n",
    "    model.add(Dropout(hp.Float('dropout3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-3, 1e-2])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                 loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='keras_tuner',\n",
    "    project_name='lstm_stock_prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data from the training set\n",
    "val_split = 0.2  # 20% of training data for validation\n",
    "val_idx = int(x_train.shape[0] * (1 - val_split))\n",
    "\n",
    "x_t = x_train[:val_idx]\n",
    "y_t = y_train[:val_idx]\n",
    "x_val = x_train[val_idx:]\n",
    "y_val = y_train[val_idx:]\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(x_t, y_t,\n",
    "            epochs=50,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1)\n",
    "\n",
    "# Get the best model\n",
    "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best hyperparameters:\\n{best_hp.values}\")\n",
    "best_model = tuner.hypermodel.build(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enhanced Training with Multiple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features we want to use for prediction\n",
    "feature_columns = ['Close', 'MA7', 'MA21', 'RSI', 'MACD', 'ROC']\n",
    "features_df = enhanced_df[feature_columns]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data = features_df.iloc[0:int(len(features_df)*0.70)]\n",
    "test_data = features_df.iloc[int(len(features_df)*0.70):]\n",
    "\n",
    "# Scale the data\n",
    "multi_feature_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_data = multi_feature_scaler.fit_transform(train_data)\n",
    "\n",
    "# Prepare the training sequences for multivariate input\n",
    "lookback = 100  # Same lookback period as before\n",
    "x_multi_train = []\n",
    "y_multi_train = []\n",
    "\n",
    "for i in range(lookback, scaled_train_data.shape[0]):\n",
    "    x_multi_train.append(scaled_train_data[i-lookback:i])\n",
    "    # We still predict the Close price (which is the first column)\n",
    "    y_multi_train.append(scaled_train_data[i, 0])\n",
    "\n",
    "x_multi_train, y_multi_train = np.array(x_multi_train), np.array(y_multi_train)\n",
    "\n",
    "print(f\"Multivariate training data shape: {x_multi_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an improved model using best hyperparameters but adapted for multivariate input\n",
    "multi_input_model = Sequential()\n",
    "\n",
    "# First LSTM layer\n",
    "multi_input_model.add(LSTM(units=best_hp.get('units_lstm1'), \n",
    "                           return_sequences=True, \n",
    "                           input_shape=(lookback, len(feature_columns))))\n",
    "multi_input_model.add(Dropout(best_hp.get('dropout1')))\n",
    "\n",
    "# Second LSTM layer\n",
    "multi_input_model.add(LSTM(units=best_hp.get('units_lstm2'), return_sequences=True))\n",
    "multi_input_model.add(Dropout(best_hp.get('dropout2')))\n",
    "\n",
    "# Third LSTM layer\n",
    "multi_input_model.add(LSTM(units=best_hp.get('units_lstm3')))\n",
    "multi_input_model.add(Dropout(best_hp.get('dropout3')))\n",
    "\n",
    "# Output layer\n",
    "multi_input_model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "multi_input_model.compile(optimizer=Adam(learning_rate=best_hp.get('learning_rate')), \n",
    "                         loss='mean_squared_error')\n",
    "\n",
    "multi_input_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the multivariate model\n",
    "history = multi_input_model.fit(\n",
    "    x_multi_train, \n",
    "    y_multi_train, \n",
    "    epochs=100, \n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating the Enhanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data for multivariate prediction\n",
    "# Scale the test data\n",
    "scaled_test_data = multi_feature_scaler.transform(test_data)\n",
    "\n",
    "# Prepare the test sequences\n",
    "x_multi_test = []\n",
    "y_multi_test = []\n",
    "\n",
    "for i in range(lookback, len(scaled_test_data)):\n",
    "    x_multi_test.append(scaled_test_data[i-lookback:i])\n",
    "    y_multi_test.append(scaled_test_data[i, 0])  # First column is Close price\n",
    "\n",
    "x_multi_test, y_multi_test = np.array(x_multi_test), np.array(y_multi_test)\n",
    "\n",
    "# Make predictions\n",
    "y_multi_pred = multi_input_model.predict(x_multi_test)\n",
    "\n",
    "# Create a dummy array to inverse transform the predictions\n",
    "multi_pred_inverse = np.zeros((len(y_multi_pred), len(feature_columns)))\n",
    "multi_test_inverse = np.zeros((len(y_multi_test), len(feature_columns)))\n",
    "\n",
    "# Place predictions and actual values in the first column (Close price)\n",
    "multi_pred_inverse[:, 0] = y_multi_pred.flatten()\n",
    "multi_test_inverse[:, 0] = y_multi_test.flatten()\n",
    "\n",
    "# Inverse transform to get the actual stock prices\n",
    "y_multi_pred_actual = multi_feature_scaler.inverse_transform(multi_pred_inverse)[:, 0]\n",
    "y_multi_test_actual = multi_feature_scaler.inverse_transform(multi_test_inverse)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate error metrics for both models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"{model_name} Model Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    print(f\"R-squared (RÂ²): {r2:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return mse, rmse, mae, mape, r2\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Comparing Model Performance:\\n\")\n",
    "original_metrics = evaluate_model(y_test, y_pred, \"Original\")\n",
    "enhanced_metrics = evaluate_model(y_multi_test_actual, y_multi_pred_actual, \"Enhanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enhanced model\n",
    "multi_input_model.save('./LSTM_model_enhanced.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Results with Advanced Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "# Get original dates for better visualization\n",
    "test_dates = test_data.index\n",
    "\n",
    "# Set the style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot actual vs predicted prices\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(test_dates[-len(y_multi_test_actual):], y_multi_test_actual, 'b-', linewidth=2, label='Actual Price')\n",
    "plt.plot(test_dates[-len(y_multi_pred_actual):], y_multi_pred_actual, 'r--', linewidth=2, label='Enhanced Model Prediction')\n",
    "plt.title('Stock Price Prediction - Enhanced Model', fontsize=16)\n",
    "plt.ylabel('Price (INR)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot prediction error\n",
    "plt.subplot(2, 1, 2)\n",
    "error = y_multi_test_actual - y_multi_pred_actual\n",
    "plt.plot(test_dates[-len(error):], error, 'g-', label='Prediction Error')\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.fill_between(test_dates[-len(error):], error, 0, alpha=0.3, color='g' if np.mean(error) < 0 else 'r')\n",
    "plt.title('Prediction Error', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Error (INR)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Forecasting Future Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forecast future stock prices\n",
    "def forecast_future_prices(model, last_sequence, n_steps, scaler, n_features):\n",
    "    \"\"\"\n",
    "    Forecast future stock prices\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    last_sequence: Last observed sequence of features used for prediction\n",
    "    n_steps: Number of future steps to predict\n",
    "    scaler: Fitted MinMaxScaler used for feature scaling\n",
    "    n_features: Number of features in the input\n",
    "    \n",
    "    Returns:\n",
    "    Array of predicted prices\n",
    "    \"\"\"\n",
    "    future_predictions = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Reshape for model input\n",
    "        current_reshape = current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1])\n",
    "        # Predict next value\n",
    "        next_pred = model.predict(current_reshape)\n",
    "        \n",
    "        # Create a sequence for inverse transform\n",
    "        dummy = np.zeros((1, n_features))\n",
    "        dummy[0, 0] = next_pred[0, 0]\n",
    "        next_price = scaler.inverse_transform(dummy)[0, 0]\n",
    "        future_predictions.append(next_price)\n",
    "        \n",
    "        # Update sequence for next prediction\n",
    "        # For simplicity, we only update the close price\n",
    "        # In a real application, you would need to generate new values for all features\n",
    "        dummy_feature_row = np.zeros((1, n_features))\n",
    "        dummy_feature_row[0, 0] = next_pred[0, 0]  # Close price\n",
    "        \n",
    "        # For simplicity, copying the last values of other features\n",
    "        # This is a simplified approach; ideally, you'd calculate new values\n",
    "        current_sequence = np.append(current_sequence[1:], [dummy_feature_row[0]], axis=0)\n",
    "    \n",
    "    return np.array(future_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last sequence from our test data\n",
    "last_sequence = x_multi_test[-1]\n",
    "\n",
    "# Forecast next 30 days\n",
    "forecast_days = 30\n",
    "forecasted_prices = forecast_future_prices(\n",
    "    multi_input_model, \n",
    "    last_sequence, \n",
    "    forecast_days, \n",
    "    multi_feature_scaler, \n",
    "    len(feature_columns)\n",
    ")\n",
    "\n",
    "# Generate future dates for plotting\n",
    "last_date = test_dates[-1]\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_days, freq='B')\n",
    "\n",
    "# Plot the historical data along with the forecasted prices\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot historical data\n",
    "historical_dates = test_dates[-60:]  # Last 60 days of historical data\n",
    "historical_prices = y_multi_test_actual[-60:]\n",
    "plt.plot(historical_dates, historical_prices, 'b-', label='Historical Prices')\n",
    "\n",
    "# Plot forecasted data\n",
    "plt.plot(future_dates, forecasted_prices, 'r--', label='Forecasted Prices')\n",
    "\n",
    "# Add a vertical line to separate historical and forecasted data\n",
    "plt.axvline(x=last_date, color='k', linestyle='-', alpha=0.5)\n",
    "plt.title('Stock Price Forecast for Next 30 Trading Days', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Price (INR)', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
